{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an Inference on Custom Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In this tutorial, we will be using a sample `RandLANet` `SemanticKITTI` weight file which we need to:\n",
    ">\n",
    "> 1. Download from: https://storage.googleapis.com/open3d-releases/model-zoo/randlanet_semantickitti_202201071330utc.pth\n",
    "> 2. Place the downloaded `randlanet_semantickitti_202201071330utc.pth` file into `'Open3D-ML/docs/tutorial/notebook/'` subdirectory, or any other place and change the `ckpt_path` accordingly.\n",
    ">\n",
    "> For other model/dataset weight files, please check out https://github.com/isl-org/Open3D-ML#semantic-segmentation-1\n",
    "\n",
    "\n",
    "An inference predicts the results based on the trained model. **Inference always depends on a prior step - _training_**.\n",
    "\n",
    "> **Please see the [Training a Semantic Segmentation Model Using PyTorch](train_ss_model_using_pytorch.ipynb) and [Training a Semantic Segmentation Model Using TensorFlow](train_ss_model_using_tensorflow.ipynb) for training tutorials.**\n",
    "\n",
    "To recap the model training process, - in our data model, we:\n",
    "\n",
    "1. define a dataset;\n",
    "2. create a training split part of the data;\n",
    "3. define a model;\n",
    "4. create a `SemanticSegmentation` `pipeline` object;\n",
    "5. execute the `pipeline.run_train()` method which runs the training process.\n",
    "\n",
    "While training, the model saves the checkpoint files every few epochs, in the *logs* directory. We use these trained weights to restore the model for inference.\n",
    "\n",
    "Our first step in Inference on a Custom Data implementation is to import *Open3D-ML* and *Numpy* libraries: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d.ml.torch as ml3d  # just switch to open3d.ml.tf for tf usage\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a checkpoint path pointing to the data weights file we downloaded (generated at the end of  the Training stage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download weights using link from model zoo (collection of weights for all combinations of model and dataset).\n",
    "# Link : https://github.com/isl-org/Open3D-ML#semantic-segmentation-1\n",
    "# Randlanet SemanticKITTI[PyTorch] : https://storage.googleapis.com/open3d-releases/model-zoo/randlanet_semantickitti_202201071330utc.pth\n",
    "ckpt_path = './randlanet_semantickitti_202201071330utc.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a `dataset` object identical to how it was done in our previous *Training a Semantic Segmentation Model* tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define dataset (similar to train_ss_using_pytorch tutorial)\n",
    "dataset = ml3d.datasets.SemanticKITTI(dataset_path='SemanticKITTI/', cache_dir='./logs/cache',training_split=['00'], validation_split=['01'], test_split=['01'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize `model` and `pipeline` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model and pipeline\n",
    "model = ml3d.models.RandLANet(in_channels=3)\n",
    "pipeline = ml3d.pipelines.SemanticSegmentation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we restore the model with our data weights file with `pipeline.load_ckpt()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-04-04 17:06:01,353 - semantic_segmentation - Loading checkpoint ./randlanet_semantickitti_202201071330utc.pth\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint using `load_ckpt` method (restoring weights for inference)\n",
    "pipeline.load_ckpt(ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets query the first pointcloud from the `test` split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-04-04 17:13:43,378 - semantickitti - Found 1 pointclouds for test\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset.get_split('test')\n",
    "data = test_data.get_data(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the content `data` contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'point': array([[ 5.2305943e+01,  2.2989707e-02,  1.9779946e+00],\n",
      "       [ 5.3259735e+01,  1.0695236e-01,  2.0099745e+00],\n",
      "       [ 5.3284321e+01,  2.7487758e-01,  2.0109341e+00],\n",
      "       ...,\n",
      "       [ 3.8249431e+00, -1.4261885e+00, -1.7655631e+00],\n",
      "       [ 3.8495324e+00, -1.4222100e+00, -1.7755738e+00],\n",
      "       [ 3.8631279e+00, -1.4142324e+00, -1.7805853e+00]], dtype=float32), 'feat': None, 'label': array([0, 0, 0, ..., 9, 9, 9], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference on custom data, you can convert your pointcloud into this format:\n",
    "\n",
    "**Dictionary with keys {'point', 'feat', 'label'}**\n",
    "\n",
    "If you already have the *ground truth labels*, you can add them to data to get accuracy and IoU (Intersection over Union). Otherwise, pass labels as `None`.\n",
    "\n",
    "And now - the main topic of our tutorial - running an inference on the test data. You can call the `run_inference()` method with your data, it will print accuracy per class and Intersection over Union (IoU) metrics. The last entry in the list is *mean accuracy* and *mean IoU*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test 0/1: 100%|██████████████████████████| 83500/83500 [09:03<00:00, 153.51it/s]\n",
      "test 0/1: 100%|████████████████████████▉| 83258/83500 [00:12<00:00, 6721.37it/s]INFO - 2022-04-04 17:28:38,137 - semantic_segmentation - Accuracy : [0.972646455436517, nan, nan, nan, nan, nan, nan, 0.0, 0.9576552007272514, 0.9159488384233881, 0.4735023041474654, nan, 0.9099061522419186, 0.9974093264248705, 0.8342307124944682, 0.8268921095008052, 0.9240506329113924, 0.9487179487179487, 0.67, 0.7859133067521687]\n",
      "INFO - 2022-04-04 17:28:38,140 - semantic_segmentation - IoU : [0.8774419082870656, nan, 0.0, nan, nan, nan, nan, 0.0, 0.900385993934381, 0.21923028864175934, 0.4655992749792312, 0.0, 0.8965375526559128, 0.08926501275214468, 0.8220736272122687, 0.7171787709497207, 0.6831550802139037, 0.8712715855572999, 0.5826086956521739, 0.5089105564882759]\n",
      "test 0/1: 100%|█████████████████████████| 83500/83500 [00:30<00:00, 6721.37it/s]"
     ]
    }
   ],
   "source": [
    "# Running inference on test data\n",
    "results = pipeline.run_inference(data)\n",
    "# prints per class accuracy and IoU (Intersection over Union). Last entry is mean accuracy and mean IoU.\n",
    "# We get several `nan` outputs for missing classes in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `results` object will return dictionary of predicted labels and predicted probabilities per point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict_labels': array([12, 12, 12, ...,  8,  8,  8], dtype=int16),\n",
       " 'predict_scores': array([[1.061e-05, 6.437e-06, 2.980e-07, ..., 5.573e-05, 1.382e-03,\n",
       "         7.524e-04],\n",
       "        [6.795e-06, 7.331e-06, 2.980e-07, ..., 7.313e-05, 2.205e-03,\n",
       "         8.492e-04],\n",
       "        [6.199e-06, 6.318e-06, 2.980e-07, ..., 7.361e-05, 3.147e-03,\n",
       "         1.205e-03],\n",
       "        ...,\n",
       "        [0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00]], dtype=float16)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary of predicted labels and predicted probabilities per class\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
