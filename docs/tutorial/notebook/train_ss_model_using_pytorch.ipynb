{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48589547",
   "metadata": {},
   "source": [
    "# Training a Semantic Segmentation Model Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b68041",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to train a semantic segmentation model using PyTorch in a Jupyter Notebook. We assume that you are familiar with Jupyter Notebook and have created a folder notebooks in a folder that is relative to ml3d.\n",
    "\n",
    "Before you begin, ensure that you have *PyTorch* installed. To install a compatible version of PyTorch, use the requirement file:\n",
    "\n",
    "```sh\n",
    "pip install -r requirements-torch-cuda.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10c2d2",
   "metadata": {},
   "source": [
    "At a high-level, we will:\n",
    "\n",
    "- Read a dataset and get a training split. For this example, we will use `SemanticKITTI` dataset.\n",
    "- Run a pre-trained model. For this example, we will use the `RandLANet` model.\n",
    "- Train a model. We will train a model using the `SemanticKITTI` dataset and `RandLANet` model.\n",
    "- Run an inference and run a test. We will run an inference using the 'training' split that use a pointcloud and display a result. However, a test is run on a pre-defined test set rather than a pass pointcloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51bcdd",
   "metadata": {},
   "source": [
    "## Reading a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69972ae",
   "metadata": {},
   "source": [
    "You can use any dataset available in the `ml3d.datasets` dataset namespace. For this example, we will use the `SemanticKITTI` dataset and visualize it. You can use any of the other dataset to load data. However, you must understand that the parameters may vary for each dataset.\n",
    "\n",
    "We will read the dataset by specifying its path and then get all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8090925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Semantic Segmentation Model using PyTorch\n",
    "\n",
    "# import torch\n",
    "import open3d.ml.torch as ml3d\n",
    "\n",
    "# Read a dataset by specifying the path. We are also providing the cache directory and training split.\n",
    "dataset = ml3d.datasets.SemanticKITTI(dataset_path='SemanticKITTI/', cache_dir='./logs/cache',training_split=['00'], validation_split=['01'], test_split=['01'])\n",
    "\n",
    "# Split the dataset for 'training'. You can get the other splits by passing 'validation' or 'test'\n",
    "train_split = dataset.get_split('training')\n",
    "\n",
    "#support of Open3d-ML visualizer in Jupyter Notebooks is in progress\n",
    "#view the frames using the visualizer\n",
    "#vis = ml3d.vis.Visualizer()\n",
    "#vis.visualize_dataset(dataset, 'training',indices=range(len(train_split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965946b",
   "metadata": {},
   "source": [
    "Now that you have visualized the dataset for training, let us train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077a5a2",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb442bcd",
   "metadata": {},
   "source": [
    "Before you train a model, you must decide which model you want to use. For this example, we will use the `RandLANet` model. To use models, you must import the model from `open3d.ml.torch.models`.\n",
    "\n",
    "After you load a dataset, you can initialize any model and then train the model. The following example shows how you can train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989a6579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Using the Open3D PyTorch ops with CUDA 11 may have stability issues!\n",
      "\n",
      " We recommend to compile PyTorch from source with compile flags\n",
      "   '-Xcompiler -fno-gnu-unique'\n",
      "\n",
      " or use the PyTorch wheels at\n",
      "   https://github.com/isl-org/open3d_downloads/releases/tag/torch1.8.2\n",
      "\n",
      "\n",
      " Ignore this message if PyTorch has been compiled with the aforementioned\n",
      " flags.\n",
      "\n",
      " See https://github.com/isl-org/Open3D/issues/3324 and\n",
      " https://github.com/pytorch/pytorch/issues/52663 for more information on this\n",
      " problem.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 12:25:57.683214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-28 12:25:57.683243: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "INFO - 2022-02-28 12:25:59,912 - semantic_segmentation - DEVICE : cpu\n",
      "INFO - 2022-02-28 12:25:59,913 - semantic_segmentation - Logging in file : ./logs/RandLANet_SemanticKITTI_torch/log_train_2022-02-28_12:25:59.txt\n",
      "INFO - 2022-02-28 12:25:59,914 - semantickitti - Found 1 pointclouds for train\n",
      "INFO - 2022-02-28 12:25:59,915 - semantickitti - Found 1 pointclouds for validation\n",
      "INFO - 2022-02-28 12:25:59,917 - semantic_segmentation - ckpt_path not given. Restore from the latest ckpt\n",
      "INFO - 2022-02-28 12:25:59,917 - semantic_segmentation - Loading checkpoint ./logs/RandLANet_SemanticKITTI_torch/checkpoint/ckpt_00000.pth\n",
      "INFO - 2022-02-28 12:25:59,979 - semantic_segmentation - Loading checkpoint optimizer_state_dict\n",
      "INFO - 2022-02-28 12:26:00,004 - semantic_segmentation - Loading checkpoint scheduler_state_dict\n",
      "INFO - 2022-02-28 12:26:00,010 - semantic_segmentation - Writing summary in train_log/00003_RandLANet_SemanticKITTI_torch.\n",
      "INFO - 2022-02-28 12:26:00,011 - semantic_segmentation - Started training\n",
      "INFO - 2022-02-28 12:26:00,013 - semantic_segmentation - === EPOCH 0/3 ===\n",
      "training:   0%|                                                                | 0/1 [00:00<?, ?it/s][W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "training: 100%|████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.85s/it]\n",
      "validation: 100%|██████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.24s/it]\n",
      "INFO - 2022-02-28 12:26:07,122 - semantic_segmentation - Loss train: 3.241  eval: 3.008\n",
      "INFO - 2022-02-28 12:26:07,123 - semantic_segmentation - Mean acc train: 0.052  eval: 0.103\n",
      "INFO - 2022-02-28 12:26:07,125 - semantic_segmentation - Mean IoU train: 0.012  eval: 0.019\n",
      "INFO - 2022-02-28 12:26:07,258 - semantic_segmentation - Epoch   0: save ckpt to ./logs/RandLANet_SemanticKITTI_torch/checkpoint\n",
      "INFO - 2022-02-28 12:26:07,259 - semantic_segmentation - === EPOCH 1/3 ===\n",
      "training: 100%|████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.67s/it]\n",
      "validation: 100%|██████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.92s/it]\n",
      "INFO - 2022-02-28 12:26:12,867 - semantic_segmentation - Loss train: 3.058  eval: 2.996\n",
      "INFO - 2022-02-28 12:26:12,868 - semantic_segmentation - Mean acc train: 0.080  eval: 0.090\n",
      "INFO - 2022-02-28 12:26:12,870 - semantic_segmentation - Mean IoU train: 0.019  eval: 0.010\n",
      "INFO - 2022-02-28 12:26:12,871 - semantic_segmentation - === EPOCH 2/3 ===\n",
      "training: 100%|████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.05s/it]\n",
      "validation: 100%|██████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "INFO - 2022-02-28 12:26:18,745 - semantic_segmentation - Loss train: 3.182  eval: 2.997\n",
      "INFO - 2022-02-28 12:26:18,747 - semantic_segmentation - Mean acc train: 0.042  eval: 0.000\n",
      "INFO - 2022-02-28 12:26:18,747 - semantic_segmentation - Mean IoU train: 0.016  eval: 0.000\n",
      "INFO - 2022-02-28 12:26:18,748 - semantic_segmentation - === EPOCH 3/3 ===\n",
      "training: 100%|████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.63s/it]\n",
      "validation: 100%|██████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "INFO - 2022-02-28 12:26:24,218 - semantic_segmentation - Loss train: 3.109  eval: 2.997\n",
      "INFO - 2022-02-28 12:26:24,219 - semantic_segmentation - Mean acc train: 0.069  eval: 0.097\n",
      "INFO - 2022-02-28 12:26:24,220 - semantic_segmentation - Mean IoU train: 0.009  eval: 0.011\n"
     ]
    }
   ],
   "source": [
    "# Training Semantic Segmentation Model using PyTorch\n",
    "\n",
    "# Import torch and the model to use for training\n",
    "import open3d.ml.torch as ml3d\n",
    "from open3d.ml.torch.models import RandLANet\n",
    "from open3d.ml.torch.pipelines import SemanticSegmentation\n",
    "\n",
    "# Read a dataset by specifying the path. We are also providing the cache directory and training split.\n",
    "# dataset = ml3d.datasets.SemanticKITTI(dataset_path='/Users/sanskara/data/SemanticKITTI/', cache_dir='./logs/cache',training_split=['00'], validation_split=['01'], test_split=['01'])\n",
    "dataset = ml3d.datasets.SemanticKITTI(dataset_path='SemanticKITTI/', cache_dir='./logs/cache',training_split=['00'], validation_split=['01'], test_split=['01'])\n",
    "\n",
    "# Initialize the RandLANet model with three layers.\n",
    "model = RandLANet(in_channels=3)\n",
    "pipeline = SemanticSegmentation(model=model, dataset=dataset, max_epoch=3, optimizer={'lr': 0.001}, num_workers=0)\n",
    "\n",
    "# Run the training\n",
    "pipeline.run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5bdba",
   "metadata": {},
   "source": [
    "## Running an Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404cae2",
   "metadata": {},
   "source": [
    "An inference processes point cloud and displays the results based on the trained model. For this example, we will use a trained `RandLANet` model.\n",
    "\n",
    "This example gets the pipeline, model, and dataset based on our previous training example. It runs the inference based the \"train\" split and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6aad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-02-28 12:28:58,179 - semantickitti - Found 1 pointclouds for test\n",
      "test 0/1:  99%|█████████████████████████████████████████████▋| 82984/83500 [00:08<00:00, 9862.05it/s]INFO - 2022-02-28 12:29:27,175 - semantic_segmentation - Accuracy : [0.0, nan, nan, nan, nan, nan, nan, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.010488245931283906, 0.7623931623931623, 0.0, 0.06440678402703719]\n",
      "INFO - 2022-02-28 12:29:27,176 - semantic_segmentation - IoU : [0.0, nan, nan, 0.0, nan, nan, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00855457227138643, 0.006550250407554818, 0.0, 0.0010789159056386606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predict_labels': array([17, 17, 17, ..., 11, 11, 11], dtype=int16), 'predict_scores': array([[0.007275, 0.00808 , 0.006298, ..., 0.007374, 0.01279 , 0.007812],\n",
      "       [0.00727 , 0.00807 , 0.0063  , ..., 0.00736 , 0.012856, 0.007786],\n",
      "       [0.007275, 0.00807 , 0.0063  , ..., 0.007355, 0.01287 , 0.007786],\n",
      "       ...,\n",
      "       [0.01765 , 0.02179 , 0.017   , ..., 0.01909 , 0.02339 , 0.02245 ],\n",
      "       [0.01765 , 0.02179 , 0.017   , ..., 0.01909 , 0.02339 , 0.02245 ],\n",
      "       [0.01765 , 0.02179 , 0.017   , ..., 0.01909 , 0.02339 , 0.02245 ]],\n",
      "      dtype=float16)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "test 0/1: 100%|██████████████████████████████████████████████| 83500/83500 [00:22<00:00, 9862.05it/s]"
     ]
    }
   ],
   "source": [
    "# Training Semantic Segmentation Model using PyTorch\n",
    "\n",
    "# Import torch and the model to use for training\n",
    "import open3d.ml.torch as ml3d\n",
    "from open3d.ml.torch.models import RandLANet\n",
    "from open3d.ml.torch.pipelines import SemanticSegmentation\n",
    "\n",
    "dataset = ml3d.datasets.SemanticKITTI(dataset_path='SemanticKITTI/', cache_dir='./logs/cache',training_split=['00'], validation_split=['01'], test_split=['01'])\n",
    "model = RandLANet(in_channels=3)\n",
    "pipeline = SemanticSegmentation(model=model, dataset=dataset, max_epoch=3, optimizer={'lr': 0.001}, num_workers=0)\n",
    "\n",
    "# Get data from the SemanticKITTI dataset using the \"train\" split\n",
    "train_split = dataset.get_split(\"test\")\n",
    "data = train_split.get_data(0)\n",
    "\n",
    "# Run the inference\n",
    "results = pipeline.run_inference(data)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb61bc0",
   "metadata": {},
   "source": [
    "## Running a Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40d3ea",
   "metadata": {},
   "source": [
    "Running a test is very similar to running an inference on Jupyter.\n",
    "\n",
    "This example gets the pipeline, model, and dataset based on our previous training example. It runs the test based the \"train\" split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9269a2df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41456/1051273395.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Get pipeline, model, and dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mPipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SemanticSegmentation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RandLANet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SemanticKITTI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_module' is not defined"
     ]
    }
   ],
   "source": [
    "# Training Semantic Segmentation Model using PyTorch\n",
    "\n",
    "# Import torch and the model to use for training\n",
    "import open3d.ml.torch as ml3d\n",
    "from open3d.ml.torch.models import RandLANet\n",
    "from open3d.ml.torch.pipelines import SemanticSegmentation\n",
    "\n",
    "# Get pipeline, model, and dataset.\n",
    "Pipeline = get_module(\"pipeline\", \"SemanticSegmentation\", \"torch\")\n",
    "Model = get_module(\"model\", \"RandLANet\", \"torch\")\n",
    "Dataset = get_module(\"dataset\", \"SemanticKITTI\")\n",
    "\n",
    "# Create a checkpoint\n",
    "RandLANet = Model(ckpt_path=args.path_ckpt_randlanet)\n",
    "SemanticKITTI = Dataset(args.path_semantickitti, use_cache=False)\n",
    "pipeline = Pipeline(model=RandLANet, dataset=SemanticKITTI)\n",
    "\n",
    "# Get data from the SemanticKITTI dataset using the \"train\" split\n",
    "train_split = SemanticKITTI.get_split(\"train\")\n",
    "data = train_split.get_data(0)\n",
    "\n",
    "# Run the test\n",
    "pipeline.run_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85be785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
