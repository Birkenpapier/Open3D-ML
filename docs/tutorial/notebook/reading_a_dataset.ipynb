{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to learn how to read Open3D-ML datasets.\n",
    "\n",
    "You can use any dataset available in the `ml3d.datasets` dataset namespace. For this example, we will use the `SemanticKITTI` dataset. You can use any of the other datasets to load data. However, you must understand that the parameters may vary for each dataset.\n",
    "\n",
    "In order to read a dataset in this example, we will supply the following parameter variables:\n",
    "\n",
    "- Dataset path (`dataset_path`)\n",
    "- Cache directory (`cache_dir`)\n",
    "- Dataset Splits (for training, validation, and testing)\n",
    "\n",
    "> **For more theoretical background information on dataset splitting, please refer to these articles:**\n",
    ">\n",
    "> https://machinelearningcompass.com/dataset_optimization/split_data_the_right_way/\n",
    ">\n",
    "> https://www.freecodecamp.org/news/key-machine-learning-concepts-explained-dataset-splitting-and-random-forest/\n",
    "\n",
    "## Creating a Global Dataset Object\n",
    "\n",
    "First, we declare the `ml3d` object to be of `open3d.ml.torch` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Using the Open3D PyTorch ops with CUDA 11 may have stability issues!\n",
      "\n",
      " We recommend to compile PyTorch from source with compile flags\n",
      "   '-Xcompiler -fno-gnu-unique'\n",
      "\n",
      " or use the PyTorch wheels at\n",
      "   https://github.com/isl-org/open3d_downloads/releases/tag/torch1.8.2\n",
      "\n",
      "\n",
      " Ignore this message if PyTorch has been compiled with the aforementioned\n",
      " flags.\n",
      "\n",
      " See https://github.com/isl-org/Open3D/issues/3324 and\n",
      " https://github.com/pytorch/pytorch/issues/52663 for more information on this\n",
      " problem.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 16:20:21.687818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-06 16:20:21.687846: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "import open3d.ml.torch as ml3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a global `dataset` object, initializing it with *path, cache directory*, and *splits*. This `dataset` consists of all files specified in the `dataset_path='<path>'` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a dataset by specifying the path. We are also providing the cache directory and splits.\n",
    "dataset = ml3d.datasets.SemanticKITTI(dataset_path='SemanticKITTI/', cache_dir='./logs/cache', training_split=['00'], validation_split=['01'], test_split=['01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of words regarding the *splits* variables: here, we isolate different portions of the `SemanticKITTI` dataset content and divide them into 3 different parts:\n",
    "\n",
    "1. `training_split` for data training. This part usually contains 70-75% of the global `dataset` content.\n",
    "2. `validation_split` for data validation. This part accounts for 10-15% of the global `dataset` content;\n",
    "3. `test_split` for testing. It contains test data and its size varies.\n",
    "\n",
    "Note the `SemanticKITTI` dataset folder structure:\n",
    "\n",
    "![dataset_splits](https://user-images.githubusercontent.com/93158890/160633509-e190d8ea-7d3c-4eea-a00f-29ba215b7e69.jpg)\n",
    "\n",
    "The three different *split* parameter variables instruct Open3D-ML subsystem to reference the following folder locations:\n",
    "\n",
    "- `training_split=['00']` points to `'SemanticKITTI/dataset/sequences/00/'`\n",
    "- `validation_split=['01']` points to `'SemanticKITTI/dataset/sequences/01/'`\n",
    "- `test_split=['01']` points to `'SemanticKITTI/dataset/sequences/01/'`\n",
    "\n",
    "> Note: **Dataset split directories usually contain numerous pointcloud files.** In our example we included only one pointcloud file for extra speed and convenience.\n",
    "\n",
    "## Creating Dataset Split Objects to Query the Data\n",
    "\n",
    "Next, we will create **dedicated** dataset split objects for specifying which split portion we would like to query.\n",
    "\n",
    "First, we create a `train_split` subset for training from the global `dataset` content we have initialized above using the `get_split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-04-06 16:22:26,972 - semantickitti - Found 1 pointclouds for training\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset for 'training'. You can get the other splits by passing 'validation' or 'test'\n",
    "train_split = dataset.get_split('training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-04-06 16:22:31,932 - semantickitti - Found 1 pointclouds for validation\n"
     ]
    }
   ],
   "source": [
    "# Similarly, get validataion split.\n",
    "val_split = dataset.get_split('validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a `test_split` subset for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-04-06 16:22:33,834 - semantickitti - Found 1 pointclouds for test\n"
     ]
    }
   ],
   "source": [
    "# Get test split\n",
    "test_split = dataset.get_split('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Size of Dataset Splits\n",
    "\n",
    "Let's see how large out *split* portions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Get length of splits\n",
    "print(len(train_split))\n",
    "print(len(val_split))\n",
    "print(len(test_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, Open3D-ML prints out the number of pointcloud files it found in `'SemanticKITTI/dataset/sequences/'`' `'/00/'` and `'/01/'` subdirectories we have specified earlier in `training_split=['00'], validation_split=['01'], test_split=['01']` varables for the `dataset`.\n",
    "\n",
    "## Querying Dataset Splits for Data\n",
    "\n",
    "In this section, we are using the `train_split` dataset split object as an example. The procedure would be identical for all other dataset splits - `val_split` and `test_split`.\n",
    "\n",
    "In order to extract the data from the `train_split`, we can iterate through the `train_split` with the index `i` (ranging from `0` - `len(train_split)-1`) using the `get_data()` method.\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'point': array([[ 5.2305943e+01,  2.2989707e-02,  1.9779946e+00],\n",
      "       [ 5.3259735e+01,  1.0695236e-01,  2.0099745e+00],\n",
      "       [ 5.3284321e+01,  2.7487758e-01,  2.0109341e+00],\n",
      "       ...,\n",
      "       [ 3.8249431e+00, -1.4261885e+00, -1.7655631e+00],\n",
      "       [ 3.8495324e+00, -1.4222100e+00, -1.7755738e+00],\n",
      "       [ 3.8631279e+00, -1.4142324e+00, -1.7805853e+00]], dtype=float32), 'feat': None, 'label': array([0, 0, 0, ..., 9, 9, 9], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "# Query splits for data, index should be from `0` to `len(split) - 1`\n",
    "for i in range(len(train_split)):\n",
    "    data = train_split.get_data(i)\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` objects from the above `for` loop return a dictionary of points (`'point'`), features (`'feat'`), and labels (`'label'`), as we will see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['point', 'feat', 'label'])\n"
     ]
    }
   ],
   "source": [
    "data = train_split.get_data(0) # Dictionary of `point`, `feat`, and `label`\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **`'point'`** key value contains a set of 3D points / coordinates - X, Y, and Z:\n",
    "\n",
    "![dataset_coordinates](https://user-images.githubusercontent.com/93158890/160503607-76e77f7a-56be-4fba-91c5-7e35019f68e8.jpg)\n",
    "\n",
    "- The **`'feat'`** (*features*) key value contains RGB color information for each of the above points.\n",
    "\n",
    "- The **`'label'`** key value represents which class the dataset content belongs to, i.e.: *pedestrian, vehicle, traffic light*, etc.\n",
    "\n",
    "### Querying Dataset Splits for Attributes\n",
    "\n",
    "We can also extract corresponding pointcloud information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'name': '00_000001', 'path': 'SemanticKITTI/dataset/sequences/00/velodyne/000001.bin', 'split': 'training'}\n"
     ]
    }
   ],
   "source": [
    "attr = train_split.get_attr(0)\n",
    "print(attr)  # Dictionary containing information about the data e.g. name, path, split, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atttributes returned are: `'idx'`(*index*), `'name'`, `'path'`, and `'split'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support of Open3d-ML visualizer in Jupyter Notebooks is in progress\n",
    "#view the frames using the visualizer\n",
    "#vis = ml3d.vis.Visualizer()\n",
    "#vis.visualize_dataset(dataset, 'training',indices=range(len(train_split)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
